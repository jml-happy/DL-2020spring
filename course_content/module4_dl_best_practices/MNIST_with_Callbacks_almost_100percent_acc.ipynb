{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_with_Callbacks_almost_100percent_acc.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/badriadhikari/2019-Spring-DL/blob/master/course_content/module4_dl_best_practices/MNIST_with_Callbacks_almost_100percent_acc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAcnDUEFtlj1",
        "colab_type": "text"
      },
      "source": [
        "## Early Stopping, Model Checkpointing & Reduce LR on Plateau - MNIST Digit Classification with Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_RwAhqCbbO9",
        "colab_type": "code",
        "outputId": "acc6532e-2450-4144-fd42-991bf9c6dce3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        }
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import *\n",
        "from keras.models import *\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "( train_images, train_labels ), ( valid_images, valid_labels ) = mnist.load_data()\n",
        "\n",
        "print('')\n",
        "print('Dataset dimensions..')\n",
        "print(train_images.shape)\n",
        "print(train_labels.shape)\n",
        "print(valid_images.shape)\n",
        "print(valid_labels.shape)\n",
        "\n",
        "print('')\n",
        "print('A sample..')\n",
        "plt.matshow( valid_images[2], cmap = 'gray')\n",
        "plt.show()\n",
        "print(valid_labels[2])\n",
        "\n",
        "print('')\n",
        "print('Preprocess..')\n",
        "train_images = train_images.reshape( ( 60000, 28, 28, 1 ) )\n",
        "train_images = train_images.astype( 'float32' ) / 255\n",
        "valid_images = valid_images.reshape( ( 10000, 28, 28, 1 ) )\n",
        "valid_images = valid_images.astype( 'float32' ) / 255\n",
        "\n",
        "print('')\n",
        "print('Convert labels to categorical..')\n",
        "train_labels = to_categorical( train_labels )\n",
        "valid_labels = to_categorical( valid_labels )\n",
        "\n",
        "print( train_labels.shape )\n",
        "print( valid_labels.shape )\n",
        "\n",
        "print('')\n",
        "print('Build model..')\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters = 16, kernel_size = 5, activation = 'relu', input_shape = (28,28,1)))\n",
        "model.add(Conv2D(filters = 16, kernel_size = 5, activation = 'relu'))\n",
        "model.add(Conv2D(filters = 16, kernel_size = 5, activation = 'relu'))\n",
        "model.add(MaxPooling2D(pool_size = 2, strides = 2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units = 16, activation = 'relu'))\n",
        "model.add(Dense(units = 10, activation = 'softmax'))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "\n",
            "Dataset dimensions..\n",
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n",
            "\n",
            "A sample..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAAECCAYAAAAYUakXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQCIQAKT/Af5qjUNr6wZYAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "\n",
            "Preprocess..\n",
            "\n",
            "Convert labels to categorical..\n",
            "(60000, 10)\n",
            "(10000, 10)\n",
            "\n",
            "Build model..\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gzk_FmPdAZa",
        "colab_type": "code",
        "outputId": "2a5c4da2-9994-4bc3-bde7-bace1587ddc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6344
        }
      },
      "source": [
        "import keras\n",
        "\n",
        "a = keras.callbacks.ModelCheckpoint( filepath='my_model.h5', monitor='val_acc', verbose = 1, save_best_only=True )\n",
        "b = keras.callbacks.EarlyStopping( monitor = 'val_acc', patience = 40, verbose = 1 )\n",
        "c = keras.callbacks.ReduceLROnPlateau( monitor = 'val_loss', factor = 0.1, patience = 20, verbose = 1 )\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Observe by setting verbose = 1\n",
        "history = model.fit(train_images, train_labels, epochs=128, batch_size=10, callbacks = [a, b, c], verbose = 1, validation_data = (valid_images, valid_labels))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/128\n",
            "60000/60000 [==============================] - 38s 634us/step - loss: 0.0249 - acc: 0.9917 - val_loss: 0.0104 - val_acc: 0.9965\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.99646, saving model to my_model.h5\n",
            "Epoch 2/128\n",
            "60000/60000 [==============================] - 31s 510us/step - loss: 0.0107 - acc: 0.9965 - val_loss: 0.0079 - val_acc: 0.9972\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.99646 to 0.99723, saving model to my_model.h5\n",
            "Epoch 3/128\n",
            "60000/60000 [==============================] - 32s 527us/step - loss: 0.0083 - acc: 0.9974 - val_loss: 0.0088 - val_acc: 0.9972\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.99723\n",
            "Epoch 4/128\n",
            "60000/60000 [==============================] - 30s 501us/step - loss: 0.0069 - acc: 0.9978 - val_loss: 0.0087 - val_acc: 0.9970\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.99723\n",
            "Epoch 5/128\n",
            "60000/60000 [==============================] - 31s 512us/step - loss: 0.0057 - acc: 0.9981 - val_loss: 0.0083 - val_acc: 0.9972\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.99723\n",
            "Epoch 6/128\n",
            "60000/60000 [==============================] - 30s 506us/step - loss: 0.0051 - acc: 0.9984 - val_loss: 0.0076 - val_acc: 0.9976\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.99723 to 0.99759, saving model to my_model.h5\n",
            "Epoch 7/128\n",
            "60000/60000 [==============================] - 31s 511us/step - loss: 0.0045 - acc: 0.9985 - val_loss: 0.0071 - val_acc: 0.9978\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.99759 to 0.99779, saving model to my_model.h5\n",
            "Epoch 8/128\n",
            "60000/60000 [==============================] - 31s 517us/step - loss: 0.0041 - acc: 0.9987 - val_loss: 0.0082 - val_acc: 0.9977\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.99779\n",
            "Epoch 9/128\n",
            "60000/60000 [==============================] - 30s 497us/step - loss: 0.0040 - acc: 0.9986 - val_loss: 0.0074 - val_acc: 0.9978\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.99779\n",
            "Epoch 10/128\n",
            "60000/60000 [==============================] - 30s 497us/step - loss: 0.0037 - acc: 0.9988 - val_loss: 0.0077 - val_acc: 0.9980\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.99779 to 0.99800, saving model to my_model.h5\n",
            "Epoch 11/128\n",
            "60000/60000 [==============================] - 33s 543us/step - loss: 0.0033 - acc: 0.9990 - val_loss: 0.0079 - val_acc: 0.9979\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.99800\n",
            "Epoch 12/128\n",
            "60000/60000 [==============================] - 30s 497us/step - loss: 0.0034 - acc: 0.9989 - val_loss: 0.0069 - val_acc: 0.9981\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.99800 to 0.99805, saving model to my_model.h5\n",
            "Epoch 13/128\n",
            "60000/60000 [==============================] - 31s 516us/step - loss: 0.0029 - acc: 0.9990 - val_loss: 0.0101 - val_acc: 0.9977\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.99805\n",
            "Epoch 14/128\n",
            "60000/60000 [==============================] - 30s 498us/step - loss: 0.0030 - acc: 0.9991 - val_loss: 0.0088 - val_acc: 0.9976\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.99805\n",
            "Epoch 15/128\n",
            "60000/60000 [==============================] - 30s 495us/step - loss: 0.0026 - acc: 0.9991 - val_loss: 0.0100 - val_acc: 0.9974\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.99805\n",
            "Epoch 16/128\n",
            "60000/60000 [==============================] - 31s 516us/step - loss: 0.0027 - acc: 0.9992 - val_loss: 0.0126 - val_acc: 0.9972\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.99805\n",
            "Epoch 17/128\n",
            "60000/60000 [==============================] - 30s 507us/step - loss: 0.0025 - acc: 0.9991 - val_loss: 0.0098 - val_acc: 0.9978\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.99805\n",
            "Epoch 18/128\n",
            "60000/60000 [==============================] - 30s 494us/step - loss: 0.0025 - acc: 0.9992 - val_loss: 0.0124 - val_acc: 0.9974\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.99805\n",
            "Epoch 19/128\n",
            "60000/60000 [==============================] - 31s 516us/step - loss: 0.0025 - acc: 0.9993 - val_loss: 0.0136 - val_acc: 0.9973\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.99805\n",
            "Epoch 20/128\n",
            "60000/60000 [==============================] - 30s 495us/step - loss: 0.0026 - acc: 0.9992 - val_loss: 0.0127 - val_acc: 0.9975\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.99805\n",
            "Epoch 21/128\n",
            "60000/60000 [==============================] - 31s 518us/step - loss: 0.0027 - acc: 0.9993 - val_loss: 0.0106 - val_acc: 0.9977\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.99805\n",
            "Epoch 22/128\n",
            "60000/60000 [==============================] - 31s 515us/step - loss: 0.0024 - acc: 0.9993 - val_loss: 0.0134 - val_acc: 0.9978\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.99805\n",
            "Epoch 23/128\n",
            "60000/60000 [==============================] - 30s 494us/step - loss: 0.0026 - acc: 0.9993 - val_loss: 0.0101 - val_acc: 0.9978\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.99805\n",
            "Epoch 24/128\n",
            "60000/60000 [==============================] - 31s 515us/step - loss: 0.0025 - acc: 0.9993 - val_loss: 0.0147 - val_acc: 0.9977\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.99805\n",
            "Epoch 25/128\n",
            "60000/60000 [==============================] - 30s 495us/step - loss: 0.0024 - acc: 0.9993 - val_loss: 0.0119 - val_acc: 0.9979\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.99805\n",
            "Epoch 26/128\n",
            "60000/60000 [==============================] - 30s 498us/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.0116 - val_acc: 0.9977\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.99805\n",
            "Epoch 27/128\n",
            "60000/60000 [==============================] - 31s 516us/step - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0145 - val_acc: 0.9977\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.99805\n",
            "Epoch 28/128\n",
            "60000/60000 [==============================] - 30s 494us/step - loss: 0.0023 - acc: 0.9994 - val_loss: 0.0179 - val_acc: 0.9977\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.99805\n",
            "Epoch 29/128\n",
            "60000/60000 [==============================] - 31s 516us/step - loss: 0.0025 - acc: 0.9994 - val_loss: 0.0136 - val_acc: 0.9980\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.99805\n",
            "Epoch 30/128\n",
            "60000/60000 [==============================] - 30s 494us/step - loss: 0.0023 - acc: 0.9994 - val_loss: 0.0183 - val_acc: 0.9978\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.99805\n",
            "Epoch 31/128\n",
            "60000/60000 [==============================] - 30s 495us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 0.0199 - val_acc: 0.9978\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.99805\n",
            "Epoch 32/128\n",
            "60000/60000 [==============================] - 32s 527us/step - loss: 0.0025 - acc: 0.9994 - val_loss: 0.0196 - val_acc: 0.9976\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.99805\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 33/128\n",
            "60000/60000 [==============================] - 30s 494us/step - loss: 7.7673e-04 - acc: 0.9998 - val_loss: 0.0150 - val_acc: 0.9980\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.99805\n",
            "Epoch 34/128\n",
            "60000/60000 [==============================] - 30s 500us/step - loss: 1.3789e-04 - acc: 1.0000 - val_loss: 0.0139 - val_acc: 0.9981\n",
            "\n",
            "Epoch 00034: val_acc improved from 0.99805 to 0.99812, saving model to my_model.h5\n",
            "Epoch 35/128\n",
            "60000/60000 [==============================] - 31s 511us/step - loss: 4.7934e-05 - acc: 1.0000 - val_loss: 0.0148 - val_acc: 0.9981\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.99812\n",
            "Epoch 36/128\n",
            "60000/60000 [==============================] - 30s 494us/step - loss: 3.5359e-05 - acc: 1.0000 - val_loss: 0.0149 - val_acc: 0.9981\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.99812\n",
            "Epoch 37/128\n",
            "60000/60000 [==============================] - 32s 528us/step - loss: 3.2345e-05 - acc: 1.0000 - val_loss: 0.0153 - val_acc: 0.9981\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.99812\n",
            "Epoch 38/128\n",
            "60000/60000 [==============================] - 30s 495us/step - loss: 3.4221e-05 - acc: 1.0000 - val_loss: 0.0155 - val_acc: 0.9980\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.99812\n",
            "Epoch 39/128\n",
            "60000/60000 [==============================] - 30s 495us/step - loss: 3.0964e-05 - acc: 1.0000 - val_loss: 0.0157 - val_acc: 0.9981\n",
            "\n",
            "Epoch 00039: val_acc improved from 0.99812 to 0.99813, saving model to my_model.h5\n",
            "Epoch 40/128\n",
            "60000/60000 [==============================] - 31s 515us/step - loss: 3.6098e-05 - acc: 1.0000 - val_loss: 0.0154 - val_acc: 0.9981\n",
            "\n",
            "Epoch 00040: val_acc improved from 0.99813 to 0.99814, saving model to my_model.h5\n",
            "Epoch 41/128\n",
            "60000/60000 [==============================] - 30s 492us/step - loss: 3.1124e-05 - acc: 1.0000 - val_loss: 0.0159 - val_acc: 0.9981\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.99814\n",
            "Epoch 42/128\n",
            "60000/60000 [==============================] - 32s 531us/step - loss: 3.0500e-05 - acc: 1.0000 - val_loss: 0.0158 - val_acc: 0.9981\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.99814\n",
            "Epoch 43/128\n",
            "60000/60000 [==============================] - 30s 500us/step - loss: 3.0304e-05 - acc: 1.0000 - val_loss: 0.0160 - val_acc: 0.9980\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.99814\n",
            "Epoch 44/128\n",
            "60000/60000 [==============================] - 30s 494us/step - loss: 3.5028e-05 - acc: 1.0000 - val_loss: 0.0160 - val_acc: 0.9981\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.99814\n",
            "Epoch 45/128\n",
            "60000/60000 [==============================] - 31s 513us/step - loss: 3.1142e-05 - acc: 1.0000 - val_loss: 0.0157 - val_acc: 0.9981\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.99814\n",
            "Epoch 46/128\n",
            "60000/60000 [==============================] - 30s 494us/step - loss: 3.4952e-05 - acc: 1.0000 - val_loss: 0.0160 - val_acc: 0.9981\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.99814\n",
            "Epoch 47/128\n",
            "60000/60000 [==============================] - 30s 506us/step - loss: 2.9749e-05 - acc: 1.0000 - val_loss: 0.0166 - val_acc: 0.9981\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.99814\n",
            "Epoch 48/128\n",
            "60000/60000 [==============================] - 31s 515us/step - loss: 3.7009e-05 - acc: 1.0000 - val_loss: 0.0162 - val_acc: 0.9981\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.99814\n",
            "Epoch 49/128\n",
            "60000/60000 [==============================] - 30s 494us/step - loss: 2.9868e-05 - acc: 1.0000 - val_loss: 0.0166 - val_acc: 0.9981\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.99814\n",
            "Epoch 50/128\n",
            "60000/60000 [==============================] - 30s 505us/step - loss: 3.6713e-05 - acc: 1.0000 - val_loss: 0.0160 - val_acc: 0.9983\n",
            "\n",
            "Epoch 00050: val_acc improved from 0.99814 to 0.99826, saving model to my_model.h5\n",
            "Epoch 51/128\n",
            "60000/60000 [==============================] - 30s 504us/step - loss: 2.9830e-05 - acc: 1.0000 - val_loss: 0.0166 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.99826\n",
            "Epoch 52/128\n",
            "60000/60000 [==============================] - 31s 522us/step - loss: 3.6536e-05 - acc: 1.0000 - val_loss: 0.0160 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.99826\n",
            "\n",
            "Epoch 00052: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Epoch 53/128\n",
            "60000/60000 [==============================] - 31s 513us/step - loss: 2.9677e-05 - acc: 1.0000 - val_loss: 0.0160 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.99826\n",
            "Epoch 54/128\n",
            "60000/60000 [==============================] - 30s 495us/step - loss: 2.9322e-05 - acc: 1.0000 - val_loss: 0.0161 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.99826\n",
            "Epoch 55/128\n",
            "60000/60000 [==============================] - 30s 495us/step - loss: 2.9323e-05 - acc: 1.0000 - val_loss: 0.0160 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.99826\n",
            "Epoch 56/128\n",
            "60000/60000 [==============================] - 31s 515us/step - loss: 2.9325e-05 - acc: 1.0000 - val_loss: 0.0161 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.99826\n",
            "Epoch 57/128\n",
            "60000/60000 [==============================] - 30s 507us/step - loss: 2.9325e-05 - acc: 1.0000 - val_loss: 0.0161 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.99826\n",
            "Epoch 58/128\n",
            "60000/60000 [==============================] - 31s 511us/step - loss: 2.9322e-05 - acc: 1.0000 - val_loss: 0.0161 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.99826\n",
            "Epoch 59/128\n",
            "60000/60000 [==============================] - 30s 499us/step - loss: 2.9329e-05 - acc: 1.0000 - val_loss: 0.0161 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.99826\n",
            "Epoch 60/128\n",
            "60000/60000 [==============================] - 30s 495us/step - loss: 2.9319e-05 - acc: 1.0000 - val_loss: 0.0161 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.99826\n",
            "Epoch 61/128\n",
            "60000/60000 [==============================] - 31s 514us/step - loss: 2.9333e-05 - acc: 1.0000 - val_loss: 0.0161 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.99826\n",
            "Epoch 62/128\n",
            "60000/60000 [==============================] - 30s 503us/step - loss: 2.9318e-05 - acc: 1.0000 - val_loss: 0.0162 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 0.99826\n",
            "Epoch 63/128\n",
            "60000/60000 [==============================] - 31s 513us/step - loss: 2.9332e-05 - acc: 1.0000 - val_loss: 0.0162 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.99826\n",
            "Epoch 64/128\n",
            "60000/60000 [==============================] - 31s 515us/step - loss: 2.9314e-05 - acc: 1.0000 - val_loss: 0.0162 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.99826\n",
            "Epoch 65/128\n",
            "60000/60000 [==============================] - 30s 494us/step - loss: 2.9339e-05 - acc: 1.0000 - val_loss: 0.0162 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.99826\n",
            "Epoch 66/128\n",
            "60000/60000 [==============================] - 31s 516us/step - loss: 2.9313e-05 - acc: 1.0000 - val_loss: 0.0162 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.99826\n",
            "Epoch 67/128\n",
            "60000/60000 [==============================] - 30s 508us/step - loss: 2.9340e-05 - acc: 1.0000 - val_loss: 0.0162 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.99826\n",
            "Epoch 68/128\n",
            "60000/60000 [==============================] - 30s 494us/step - loss: 2.9313e-05 - acc: 1.0000 - val_loss: 0.0162 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.99826\n",
            "Epoch 69/128\n",
            "60000/60000 [==============================] - 31s 515us/step - loss: 2.9340e-05 - acc: 1.0000 - val_loss: 0.0162 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.99826\n",
            "Epoch 70/128\n",
            "60000/60000 [==============================] - 30s 496us/step - loss: 2.9312e-05 - acc: 1.0000 - val_loss: 0.0163 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.99826\n",
            "Epoch 71/128\n",
            "60000/60000 [==============================] - 30s 494us/step - loss: 2.9341e-05 - acc: 1.0000 - val_loss: 0.0162 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.99826\n",
            "Epoch 72/128\n",
            "60000/60000 [==============================] - 31s 515us/step - loss: 2.9312e-05 - acc: 1.0000 - val_loss: 0.0163 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.99826\n",
            "\n",
            "Epoch 00072: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "Epoch 73/128\n",
            "60000/60000 [==============================] - 31s 520us/step - loss: 2.9339e-05 - acc: 1.0000 - val_loss: 0.0163 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.99826\n",
            "Epoch 74/128\n",
            "60000/60000 [==============================] - 31s 517us/step - loss: 2.9320e-05 - acc: 1.0000 - val_loss: 0.0163 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 0.99826\n",
            "Epoch 75/128\n",
            "60000/60000 [==============================] - 30s 493us/step - loss: 2.9304e-05 - acc: 1.0000 - val_loss: 0.0163 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 0.99826\n",
            "Epoch 76/128\n",
            "60000/60000 [==============================] - 30s 494us/step - loss: 2.9291e-05 - acc: 1.0000 - val_loss: 0.0163 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00076: val_acc did not improve from 0.99826\n",
            "Epoch 77/128\n",
            "60000/60000 [==============================] - 31s 515us/step - loss: 2.9283e-05 - acc: 1.0000 - val_loss: 0.0163 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00077: val_acc did not improve from 0.99826\n",
            "Epoch 78/128\n",
            "60000/60000 [==============================] - 30s 493us/step - loss: 2.9278e-05 - acc: 1.0000 - val_loss: 0.0163 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00078: val_acc did not improve from 0.99826\n",
            "Epoch 79/128\n",
            "60000/60000 [==============================] - 30s 494us/step - loss: 2.9277e-05 - acc: 1.0000 - val_loss: 0.0163 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00079: val_acc did not improve from 0.99826\n",
            "Epoch 80/128\n",
            "60000/60000 [==============================] - 31s 513us/step - loss: 2.9278e-05 - acc: 1.0000 - val_loss: 0.0163 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 0.99826\n",
            "Epoch 81/128\n",
            "60000/60000 [==============================] - 30s 492us/step - loss: 2.9277e-05 - acc: 1.0000 - val_loss: 0.0163 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 0.99826\n",
            "Epoch 82/128\n",
            "60000/60000 [==============================] - 31s 515us/step - loss: 2.9278e-05 - acc: 1.0000 - val_loss: 0.0163 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 0.99826\n",
            "Epoch 83/128\n",
            "60000/60000 [==============================] - 31s 519us/step - loss: 2.9277e-05 - acc: 1.0000 - val_loss: 0.0163 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.99826\n",
            "Epoch 84/128\n",
            "60000/60000 [==============================] - 30s 493us/step - loss: 2.9278e-05 - acc: 1.0000 - val_loss: 0.0163 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00084: val_acc did not improve from 0.99826\n",
            "Epoch 85/128\n",
            "60000/60000 [==============================] - 31s 513us/step - loss: 2.9277e-05 - acc: 1.0000 - val_loss: 0.0163 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00085: val_acc did not improve from 0.99826\n",
            "Epoch 86/128\n",
            "60000/60000 [==============================] - 30s 492us/step - loss: 2.9278e-05 - acc: 1.0000 - val_loss: 0.0163 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00086: val_acc did not improve from 0.99826\n",
            "Epoch 87/128\n",
            "60000/60000 [==============================] - 30s 508us/step - loss: 2.9277e-05 - acc: 1.0000 - val_loss: 0.0163 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00087: val_acc did not improve from 0.99826\n",
            "Epoch 88/128\n",
            "60000/60000 [==============================] - 31s 510us/step - loss: 2.9278e-05 - acc: 1.0000 - val_loss: 0.0163 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.99826\n",
            "Epoch 89/128\n",
            "60000/60000 [==============================] - 30s 492us/step - loss: 2.9277e-05 - acc: 1.0000 - val_loss: 0.0163 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00089: val_acc did not improve from 0.99826\n",
            "Epoch 90/128\n",
            "60000/60000 [==============================] - 31s 515us/step - loss: 2.9278e-05 - acc: 1.0000 - val_loss: 0.0163 - val_acc: 0.9982\n",
            "\n",
            "Epoch 00090: val_acc did not improve from 0.99826\n",
            "Epoch 00090: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKUU7VchEDrW",
        "colab_type": "code",
        "outputId": "99407f6d-1e43-4c91-c351-edce87f6356a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUNG8ThFxQQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLG0G3PsdDkm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "61d4bf70-9e95-4911-88bc-47b8e4adb999"
      },
      "source": [
        "scores = model.evaluate(valid_images, valid_labels)\n",
        "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 60us/step\n",
            "\n",
            "acc: 99.82%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AbdIEs2dJqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('my_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTB_P4mGfGIe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9b3698ed-8298-4da5-c30b-fc06cc91e992"
      },
      "source": [
        "scores = model.evaluate(valid_images, valid_labels)\n",
        "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 65us/step\n",
            "\n",
            "acc: 99.83%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA1CbigdfT8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"my_model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r23xwVc0Zcqy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}